---
title: "2 Training and Model Selection"
author: "Paul Hively"
date: "April 14, 2016"
output: html_document
---

Source can be viewed [on GitHub](https://github.com/phively/gsb-sols-proj/blob/master/2%20Training%20and%20model%20selection.Rmd)

# Logistic modeling

## First attempt

Begin by loading the data used for the visualizations in part 1.

```{r, message=F, warning=F, cache=F}
#### Run Rscript0 to load useful packages and functions ----
source("Rscript0 - libraries.R")
source("f.Wrangle.R")
```
```{r, message=F, warning=F, cache=F}
#### Model data cleanup ----
source("Rscript1a - modeling data appends.R")
```

* [Rscript0 - libraries.R](https://github.com/phively/gsb-sols-proj/blob/master/Rscript0%20-%20libraries.R)
* [f.Wrangle.R](https://github.com/phively/gsb-sols-proj/blob/master/f.Wrangle.R)
* [Rscript1a - modeling data appends.R](https://github.com/phively/gsb-sols-proj/blob/master/Rscript1a%20-%20modeling%20data%20appends.R)

Recall:

> The existing model ("JMod") has two components. First, the probability a solicitation $S_{i}$ comes in before the end of the fiscal year is a function of the current stage progress $c$ and time until the end of the year $t$, $P(S_{i}=1)=f(c_{i},t)$. The expected FRP in the current fiscal year is this probability times the expected amount, $E(FRP)=P(S_{i}=1)E(S_{i})$.
>
> Focus on the probability model. I assume that there should also be some effect due to the average close rate, season (e.g. people give more in December), planned ask amount, actual ask amount, and potentially interactions. Something like this:
>
> $$P(S_{ij}=1)=f(c_{i},t,\mu,t_{i}^{*},a_{j}^{*},a_{j})$$

Start by trying to directly predict the booked in FY probabilities. The simplest approach is to treat everything that isn't continuous as a factor and chuck it into a logistic regression model.

$$logit(p)=log\Big(\frac{p}{1-p}\Big)=\eta$$
$$E(\eta_{i})=c_{i}+t_{i}^{*}$$

In other words, the logit of the expected close rate is dependent on some intercept $c_{i}$ due to stage, and some month effect $t_{i}^{*}$.

```{r, echo=F, message=F, warning=F, cache=F}
## Function to examine the deviances to assess goodness of fit
dev.comp <- function(models, modnames, debug=F){
  # Loop through the models passed to the function
  out <- NULL
  # Check if not actually passed a list of models
  is.list <- class(models)[1]=="list"
    if(debug){print(is.list)}
  if(is.list) {n=length(models)} else {n=1}
    if(debug){print(paste("n =",n))}
  # Loop through models
  for(i in 1:n){
    # Extract the current model
    if(is.list) {m <- summary(models[[i]])} else {m <- summary(models)}
    m$difference = m$null.deviance - m$deviance
    m$rel.difference = m$difference / m$null.deviance
    m$df.p = m$df.null
    m$df.q = m$df.residual
    m$p = with(m, pchisq(difference, df=df.p-df.q, lower.tail=F))
    out <- rbind(out, data.frame(null.dev=m$null.deviance, dev=m$deviance, diff=m$difference, rel.diff=m$rel.difference, df.p=m$df.p, df.q=m$df.q, p=m$p))
    if(debug){print(out)}
  }
  rownames(out) <- modnames
  return(out)
}

## Function to generate confusion matrix
confusion <- function(models, threshold, modnames, counts=T, digits=2, debug=F){
  out <- list()
  # Check if not actually passed a list of models
  is.list <- class(models)[1]=="list"
    if(debug){print(is.list)}
  if(is.list) {n=length(models)} else {n=1}
    if(debug){print(paste("n =",n))}
  # Loop through models
  for(i in 1:n){
    # Extract the current model
    if(is.list) {m <- models[[i]]} else {m <- models}
    tabl <- table(truth=model.frame(m)[,1], pred=fitted(m)>=threshold)
    if(counts) {out[[i]] <- addmargins(tabl)}
    if(!counts){out[[i]] <- signif(addmargins(prop.table(tabl)), digits)}
  }
  names(out) <- modnames
  return(out)
}

## Function to return binomial error vectors

```
```{r, message=F, warning=F, cache=F}
## Model each stage by when it was asked 
glm.pln.1 <- glm(FY.Plan.Book ~ as.factor(month(mdat$Planning.Dt)), data=mdat, family=binomial())
glm.clr.1 <- glm(FY.Clear.Book ~ as.factor(month(mdat$Planning.Dt)), data=mdat, family=binomial())
glm.ask.1 <- glm(FY.Ask.Book ~ as.factor(month(mdat$Planning.Dt)), data=mdat, family=binomial())
glm.ora.1 <- glm(FY.Oral.Book ~ as.factor(month(mdat$Planning.Dt)), data=mdat, family=binomial())
modnames <- c("Plan","Clear","Ask","Oral")
# Compare each of the models
kable(dev.comp(models=list(glm.pln.1, glm.clr.1, glm.ask.1, glm.ora.1), modnames=modnames, debug=F), digits=3)
```

Month-by-month coefficients result in an unambiguously better model compared to just an intercept per stage.

JMod has the following structure. Cell entries are probability to close given that a solicitation is in the indicated stage and month, with an expected close date that fiscal year:

```{r, echo=F, message=F, warning=F, cache=F}
library(MASS) #to write results in fractional form
fractions(matrix(c(1/6,1/3,2/3,1, 1/8,1/6,1/3,1, 0,1/8,1/6,1, 0,0,1/8,1), nrow=4, dimnames=list(c(Stage=c("Plan","Clear","Ask","Oral")), c("July.to.Feb", "Mar.to.Apr", "May", "June"))))
detach(package:MASS) #clean up because MASS::select conflicts with dplyr::select
```

Here are the confusion matrices, with a classification threshold of $p=.5$:

```{r, message=F, warning=F, cache=F}
print("Logistic models")
# Count tables
confusion(list(glm.pln.1, glm.clr.1, glm.ask.1, glm.ora.1), threshold=.5, modnames=modnames)
# Proportion tables
confusion(list(glm.pln.1, glm.clr.1, glm.ask.1, glm.ora.1), threshold=.5, modnames=modnames, counts=F)
```

Compare errors for JMod versus the month intercepts logistic model by stage. My error function is

$$\epsilon_{i} = \left|p_{pred}-p_{actual}\right|$$

```{r, message=F, warning=F, cache=F}
## Load functions for JMod
source("f.JMod.R")
```

* [f.JMod.R](https://github.com/phively/gsb-sols-proj/blob/master/f.JMod.R)

```{r, echo=F, message=F, warning=F, cache=F}
## Planning stage
# Residuals
jmod <- JMod.err(probs=JMod(curr.stage=1, curr.dt=mdat$Planning.Dt, expected.dt=mdat$Expected.Dt)$probability, truth=mdat$FY.Plan.Book)
glmod <- JMod.err(probs=glm.pln.1$fitted, truth=model.frame(glm.pln.1)[,1])
# Combined residuals
print("Plan absolute residual sum")
data.frame(jmod=sum(abs(jmod)), glm=sum(abs(glmod)))
ggdat <- data.frame(jmod, glm=glmod, closed=mdat$FY.Plan.Book)
ggplot(ggdat, aes(x=glm, y=jmod, color=closed)) + geom_point() + geom_hline(yintercept=mean(jmod), alpha=.3) + geom_vline(xintercept=mean(glmod), alpha=.3) + labs(title="Plan JMod versus glm residual error and bias")
```

JMod wins on the sum of absolute residuals, but it is very negatively biased, to the tune of $-0.325$.

```{r, echo=F, message=F, warning=F, cache=F}
## Clear stage
# Residuals
jmod <- na.omit(JMod.err(probs=JMod(curr.stage=2, curr.dt=mdat$Clear.Dt, expected.dt=mdat$Expected.Dt)$probability, truth=mdat$FY.Clear.Book))
glmod <- JMod.err(probs=glm.clr.1$fitted, truth=model.frame(glm.clr.1)[,1])
# Combined residuals
print("Clear absolute residual sum")
data.frame(jmod=sum(abs(jmod)), glm=sum(abs(glmod)))
ggdat <- data.frame(jmod, glm=glmod, closed=na.omit(mdat$FY.Clear.Book))
ggplot(ggdat, aes(x=glm, y=jmod, color=closed)) + geom_point() + geom_hline(yintercept=mean(jmod), alpha=.3) + geom_vline(xintercept=mean(glmod), alpha=.3) + labs(title="Clear JMod versus glm residual error and bias")
```

A tiny bit closer on the sum of absolute residuals, but JMod is still negatively biased, around $-0.295$

```{r, echo=F, message=F, warning=F, cache=F}
## Ask stage
# Residuals
jmod <- na.omit(JMod.err(probs=JMod(curr.stage=3, curr.dt=mdat$Ask.Dt, expected.dt=mdat$Expected.Dt)$probability, truth=mdat$FY.Ask.Book))
glmod <- JMod.err(probs=glm.ask.1$fitted, truth=model.frame(glm.ask.1)[,1])
# Combined residuals
print("Ask absolute residual sum")
data.frame(jmod=sum(abs(jmod)), glm=sum(abs(glmod)))
ggdat <- data.frame(jmod, glm=glmod, closed=na.omit(mdat$FY.Ask.Book))
ggplot(ggdat, aes(x=glm, y=jmod, color=closed)) + geom_point() + geom_hline(yintercept=mean(jmod), alpha=.3) + geom_vline(xintercept=mean(glmod), alpha=.3) + labs(title="Ask JMod versus glm residual error and bias")
```

Clear win for JMod; bias is only $-0.104$.

```{r, echo=F, message=F, warning=F, cache=F}
## Oral stage
# Residuals
jmod <- na.omit(JMod.err(probs=JMod(curr.stage=4, curr.dt=mdat$Oral.Dt, expected.dt=mdat$Expected.Dt)$probability, truth=mdat$FY.Oral.Book))
glmod <- JMod.err(probs=glm.ora.1$fitted, truth=model.frame(glm.ora.1)[,1])
# Combined residuals
print("Oral absolute residual sum")
data.frame(jmod=sum(abs(jmod)), glm=sum(abs(glmod)))
ggdat <- data.frame(jmod, glm=glmod, closed=na.omit(mdat$FY.Oral.Book))
ggplot(ggdat, aes(x=glm, y=jmod, color=closed)) + geom_point() + geom_hline(yintercept=mean(jmod), alpha=.3) + geom_vline(xintercept=mean(glmod), alpha=.3) + labs(title="Oral JMod versus glm residual error and bias")
```

JMod bias is $0.047$.

Basically, what this comes down to is how concerned are we about bias? The negative bias is due to overaggressive discounting as the end of the fiscal year (June 30) approaches.

From [part 1](https://github.com/phively/gsb-sols-proj/blob/master/1%20Data%20exploration%20commentary.Rmd), some interesting variables include **expected amount**, and whether **ask amount/planned ask** is greater or less than 1. Potentially interesting derived variables include the **number of days left in the year**, the **number of days until expected close**, and the **amount of time that's passed since the previous stage**.

Some other thoughts.

1. Is logistic regression really suitable (overdispersion)?
2. Should it be restricted to MG-range gifts, $[\text{$}25\text{k},\text{$}5\text{M})$?
3. What nonparametric methods are worth trying?

# Variable selection

Create those derived variables.

```{r, message=F, warning=F, cache=F}
## Derived variables

```

<Insert GitHub link>

Think about #3 first. I'm a fan of classification trees, and that's closest to the existing approach. It would also be interesting to see which of the proposed variables end up being chosen.

Try the random forest approach to select variables....

```{r, message=F, warning=F, cache=F}
library(randomForest)
```



# Packages used
```{r}
session_info()
```